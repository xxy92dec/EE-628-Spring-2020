{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Preparation\n",
    "\n",
    "Before introducing the model, let’s assume we will use a neural network to train a language model.Now the question is how to read mini-batches of examples and labels at random. We describe how to accomplish this for both random\n",
    "sampling and sequential partitioning strategies below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling\n",
    "\n",
    "The following code randomly generates a minibatch from the data each time. Here, the batch size batch_size\n",
    "indicates to the number of examples in each mini-batch and num_steps is the length of the sequence (or time\n",
    "steps if we have a time series) included in each example. In random sampling, each example is a sequence\n",
    "arbitrarily captured on the original sequence. The positions of two adjacent random mini-batches on the\n",
    "original sequence are not necessarily adjacent. The target is to predict the next character based on what\n",
    "we’ve seen so far, hence the labels are the original sequence, shifted by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = utils.load_corpus_time_machine(max_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 2, 1, 3, 5, 13, 2, 1, 13]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    # Offset the iterator over the data for uniform starts\n",
    "    corpus = corpus[random.randint(0, num_steps):]\n",
    "    # Subtract 1 extra since we need to account for label\n",
    "    num_examples = (len(corpus) - 1)//num_steps\n",
    "    example_indices = list(range(0, num_examples*num_steps,\n",
    "                                num_steps))\n",
    "    random.shuffle(example_indices)\n",
    "    # This returns a sequence of the length num_steps starting from pos\n",
    "    data = lambda pos: corpus[pos:pos+num_steps]\n",
    "    # Discard half empty batches\n",
    "    num_batches = num_examples // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # Batch_size indicates the random examples read each time\n",
    "        batch_indices = example_indices[i:i+batch_size]\n",
    "        X = [data(j) for j in batch_indices]\n",
    "        Y = [data(j+1) for j in batch_indices]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate an artificial sequence from 0 to 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10]])\n",
      "Y: tensor([[ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "X: tensor([[23, 24, 25, 26],\n",
      "        [19, 20, 21, 22]])\n",
      "Y: tensor([[24, 25, 26, 27],\n",
      "        [20, 21, 22, 23]])\n",
      "X: tensor([[15, 16, 17, 18],\n",
      "        [11, 12, 13, 14]])\n",
      "Y: tensor([[16, 17, 18, 19],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "# type your code here\n",
    "my_seq = list(range(30))\n",
    "for X, Y in seq_data_iter_random(my_seq, batch_size=2,\n",
    "                                num_steps=4):\n",
    "    print('X:', X) \n",
    "    print('Y:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to random sampling of the original sequence, we can also make the positions of two adjacent\n",
    "random mini-batches adjacent in the original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_consecutive(corpus, batch_size, num_steps):\n",
    "    # Offset for the iterator over the data for uniform starts\n",
    "    offset = random.randint(0, num_steps)\n",
    "    # Slice out data - ignore num_steps and just wrap around\n",
    "    num_indices = ((len(corpus) - offset - 1)//batch_size\n",
    "                   *batch_size)\n",
    "    Xs = torch.tensor(corpus[offset: offset+num_indices])\n",
    "    Ys = torch.tensor(corpus[offset+1: offset+1+num_indices])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_batches * num_steps, num_steps):\n",
    "        X = Xs[:,i:(i+num_steps)]\n",
    "        Y = Ys[:, i:(i+num_steps)]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same settings, print input X and label Y for each mini-batch of examples read by r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 2,  3,  4,  5],\n",
      "        [15, 16, 17, 18]])\n",
      "Y: tensor([[ 3,  4,  5,  6],\n",
      "        [16, 17, 18, 19]])\n",
      "X: tensor([[ 6,  7,  8,  9],\n",
      "        [19, 20, 21, 22]])\n",
      "Y: tensor([[ 7,  8,  9, 10],\n",
      "        [20, 21, 22, 23]])\n",
      "X: tensor([[10, 11, 12, 13],\n",
      "        [23, 24, 25, 26]])\n",
      "Y: tensor([[11, 12, 13, 14],\n",
      "        [24, 25, 26, 27]])\n"
     ]
    }
   ],
   "source": [
    "# type your code here\n",
    "my_seq = list(range(30))\n",
    "for X, Y in seq_data_iter_consecutive(my_seq, batch_size=2,\n",
    "                                     num_steps=4):\n",
    "    print('X:', X)\n",
    "    print('Y:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap the above two sampling functions to a class so that we can use it as a normal pytorch data iterator later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader(object):\n",
    "    \"\"\"A iterator to load sequence data\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            data_iter_fn = seq_data_iter_consecutive\n",
    "        self.corpus, self.vocab = utils.load_corpus_time_machine(max_tokens)\n",
    "        self.get_iter = lambda: data_iter_fn(self.corpus, \n",
    "                                             batch_size,\n",
    "                                            num_steps)\n",
    "    def __iter__(self):\n",
    "        return self.get_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define a function load_data_time_machine that returns both the data iterator and the vocabulary,\n",
    "so we can use it similarly as other functions with load_data prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_time_machine(batch_size, num_steps, \n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps,\n",
    "                             use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
